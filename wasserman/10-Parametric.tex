\section*{10. Parametric Inference}\label{parametric-inference}
\textbf{Parametric models} are of the form
\[
\mathfrak{F} = \bigg\{ f(x; \theta) : \; \theta \in \Theta \bigg\}
\]
where \(\Theta \subset \R^{k}\) is the parameter space and
\(\theta = (\theta_{1}, \dots, \theta_{k})\) is the parameter. The problem
of inference then reduces to the problem of estimating parameter
\(\theta\).

\subsection*{10.1 Parameter of interest}\label{parameter-of-interest}
Often we are only interested in some function \(T(\theta)\). For
example, if \(X \sim N(\mu, \sigma^{2})\) then the parameter is
\(\theta = (\mu, \sigma)\). If our goal is to estimate \(\mu\) then
\(\mu = T(\theta)\) is called the \textbf{parameter of interest} and
\(\sigma\) is called a \textbf{nuisance parameter}.

\subsection*{10.2 The Method of Moments}\label{the-method-of-moments}
Suppose that the parameter \(\theta = (\theta_{1}, \dots, \theta_{n})\) has
\(k\) components. For \(1 \leq j \leq k\) define the \(j\)-th
\textbf{moment}
\[
\alpha_{j} \equiv \alpha_{j}(\theta) = \EXP_{\theta}(X^{j}) = \int x^{j} dF_\theta(x)
\]
and the \(j\)-th \textbf{sample moment}
\[
\hat{\alpha}_{j} = \frac{1}{n} \sum_{i=1}^{n} X_{i}^{j}
\]
The \textbf{method of moments estimator} \(\hat{\theta}_{n}\) is defined
to be the value of \(\theta\) such that
\begin{align*}
\alpha_{1}(\hat{\theta}_{n}) &= \hat{\alpha}_{1} \\
\alpha_{2}(\hat{\theta}_{n}) &= \hat{\alpha}_{2} \\
\vdots \\
\alpha_{k}(\hat{\theta}_{n}) &= \hat{\alpha}_{k}  
\end{align*}
This defines a system of \(k\) equations with \(k\) unknowns.

\textbf{Theorem 10.6}. Let \(\hat{\theta}_{n}\) denote the method of
moments estimator. Under the conditions given in the appendix, the
following statements hold:
\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  The estimate \(\hat{\theta}_{n}\) exists with probability tending to 1.
\item
  The estimate is consistent:
  \(\hat{\theta}_{n} \xrightarrow{\textrm{P}} \theta\).
\item
  The estimate is asymptotically Normal:
\end{enumerate}
\[
\sqrt(n)(\hat{\theta}_{n} - \theta) \leadsto N(0, \Sigma)
\]
where
\[
\Sigma = g \EXP_{\theta} (Y Y^T) g^T \\
Y = (X, X^{2}, \dots, X^{k})^T, \quad g = (g_{1}, \dots, g_{k}) \quad \text{and} \quad g_{j} = \partial \alpha_{j}^{-1}(\theta)/\partial\theta
\]
The last statement in Theorem 10.6 can be used to find standard errors
and confidence intervals. However, there is an easier way: the
bootstrap.

\subsection*{10.3 Maximum Likelihood}\label{maximum-likelihood}
Let \(X_{1}, \dots, X_{n}\) be iid with PDF \(f(x; \theta)\).
The \textbf{likelihood function} is defined by
\[
\mathcal{L}_{n}(\theta) = \prod_{i=1}^{n} f(X_{i}; \theta)
\]
The \textbf{log-likelihood function} is defined by
\(\ell_{n}(\theta) = \log \mathcal{L}_{n}(\theta)\).
The likelihood function is just the joint density of the data, except we
treat is as a function of parameter \(\theta\). Thus
\(\mathcal{L}_{n} : \Theta \rightarrow [0, \infty)\). The likelihood
function is not a density function; in general it is not true that
\(\mathcal{L}_{n}\) integrates to 1.
The \textbf{maximum likelihood estimator} MLE, denoted by
\(\hat{\theta}_{n}\), is the value of \(\theta\) that maximizes
\(\mathcal{L}_{n}(\theta)\).
The maximum of \(\ell_{n}(\theta)\) occurs at the same place as the
maximum of \(\mathcal{L}_{n}(\theta)\), so maximizing either leads to the
same answer. Often it is easier to maximize the log-likelihood.

\subsection*{10.4 Properties of Maximum Likelihood
Estimators}\label{properties-of-maximum-likelihood-estimators}
Under certain conditions on the model, the MLE \(\hat{\theta}_{n}\)
possesses many properties that make it an appealing choice of estimator.
The main properties of the MLE are:
\begin{itemize}[tightlist]
\item
  It is \textbf{consistent}:
  \(\hat{\theta}_{n} \xrightarrow{\textrm{P}} \theta_{*}\), where \(\theta_{*}\)
  denotes the true value of parameter \(\theta\).
\item
  It is \textbf{equivariant}: if \(\hat{\theta}_{n}\) is the MLE of
  \(\theta\) then \(g(\hat{\theta}_{n})\) is the MLE of \(g(\theta)\).
\item
  If is \textbf{asymptotically Normal}:
  \(\sqrt{n}(\hat{\theta} - \theta_{*}) / \widehat{\SE} \leadsto N(0, 1)\)
  where \(\widehat{\SE}\) can be computed analytically.
\item
  It is \textbf{asymptotically optimal} or \textbf{efficient}: roughly,
  this means that among all well behaved estimators, the MLE has the
  smallest variance, at least for large samples.
\item
  The MLE is approximately the Bayes estimator.
\end{itemize}

\subsection*{10.5 Consistency of Maximum Likelihood
Estimator}\label{consistency-of-maximum-likelihood-estimator}
If \(f\) and \(g\) are PDFs, define the \textbf{Kullback-Leibler
distance} between \(f\) and \(g\) to be:
\[
D(f, g) = \int f(x) \log \left( \frac{f(x)}{g(x)} \right) dx
\]
It can be shown that \(D(f, g) \geq 0\) and \(D(f, f) = 0\). For any
\(\theta, \psi \in \Theta\) write \(D(\theta, \psi)\) to mean
\(D(f(x; \theta), f(x; \psi))\). We will assume that
\(\theta \neq \psi\) implies \(D(\theta, \psi) > 0\).
Let \(\theta_{*}\) denote the true value of \(\theta\). Maximizing
\(\ell_{n}(\theta)\) is equivalent to maximizing
\[
M_{n}(\theta) = \frac{1}{n} \sum_{i} \log \frac{f(X_{i}; \theta)}{f(X_{i}; \theta_{*})}
\]
By the law of large numbers, \(M_{n}(\theta)\) converges to:
\begin{align*}
\EXP_{\theta_{*}} \left( \log \frac{f(X_{i}; \theta)}{f(X_{i}; \theta_{*})} \right)
& = \int \log \left( \frac{f(x; \theta)}{f(x; \theta_{*})} \right) f(x; \theta_{*}) dx \\
& = - \int \log \left( \frac{f(x; \theta_{*})}{f(x; \theta)} \right) f(x; \theta_{*}) dx \\
&= -D(\theta_{*}, \theta)
\end{align*}
Hence \(M_{n}(\theta) \approx -D(\theta_{*}, \theta)\) which is maximized at
\(\theta_{*}\), since the KL distance is 0 when \(\theta_{*} = \theta\) and
positive otherwise. Hence, we expect that the maximizer will tend to
\(\theta_{*}\).
To prove this formally, we need more than
\(M_{n}(\theta) \xrightarrow{\textrm{P}} -D(\theta_{*}, \theta)\). We need
this convergence to be uniform over \(\theta\). We also have to make
sure that the KL distance is well-behaved. Here are the formal details.

\textbf{Theorem 10.13}. Let \(\theta_{*}\) denote the true value of
\(\theta\). Define
\[
M_{n}(\theta) = \frac{1}{n} \sum_{i} \log \frac{f(X_{i}; \theta)}{f(X_{i}; \theta_{*})}
\]
and \(M(\theta) = -D(\theta_{*}, \theta)\). Suppose that
\[
\sup _{\theta \in \Theta} |M_{n}(\theta) - M(\theta)| \xrightarrow{\textrm{P}} 0
\]
and that, for every \(\epsilon > 0\),
\[
\sup _{\theta : |\theta - \theta_{*}| \geq \epsilon} M(\theta) < M(\theta_{*})
\]
Let \(\hat{\theta}_{n}\) denote the mle. Then
\(\hat{\theta}_{n} \xrightarrow{\textrm{P}} \theta_{*}\).

\subsection*{10.6 Equivalence of the
MLE}\label{equivalence-of-the-mle}

\textbf{Theorem 10.14}. Let \(\tau = g(\theta)\) be a one-to-one
function of \(\theta\). Let \(\hat{\theta}_{n}\) be the MLE of \(\theta\).
Then \(\hat{\tau}_{n} = g(\hat{\theta}_{n})\) is the MLE of \(\tau\).
\textbf{Proof}. Let \(h = g^{-1}\) denote the inverse of \(g\). Then
\(\hat{\theta}_{n} = h(\hat{\tau}_{n})\). For any \(\tau\),
\(L(\tau) = \prod_{i} f(x_{i}; h(\tau)) = \prod_{i} f(x_{i}; \theta) = \mathcal{L}(\theta)\)
where \(\theta = h(\tau)\). Hence, for any \(\tau\),
\(\mathcal{L}_{n}(\tau) = \mathcal{L}(\theta) \leq \mathcal{L}(\hat{\theta}) = \mathcal{L}_{n}(\hat{\tau})\).

\subsection*{10.7 Asymptotic Normality}\label{asymptotic-normality}
The \textbf{score function} is defined to be
\[
s(X; \theta) = \frac{\partial \log f(X; \theta)}{\partial \theta}
\]
The \textbf{Fisher information} is defined to be
\begin{align*}
I_{n}(\theta) &= \VAR_\theta \left( \sum_{i=1}^{n} s(X_{i}; \theta) \right) \\
&= \sum_{i=1}^{n} \VAR_\theta(s(X_{i}; \theta))
\end{align*}
For \(n = 1\) we sometimes write \(I(\theta)\) instead of
\(I_{1}(\theta)\).
It can be shown that \(\EXP_{\theta}(s(X; \theta)) = 0\). It then
follows that
\(\VAR_\theta(s(X; \theta)) = \EXP_{\theta}((s(X; \theta))^{2})\).
A further simplification of \(I_{n}(\theta)\) is given in the next result.

\textbf{Theorem 10.17}.
\[
I_{n}(\theta) = n I(\theta)
\]
\begin{align*}
I(\theta) & = -\EXP_{\theta} \left( \frac{\partial^{2} \log f(X; \theta)}{\partial^{2} \theta^{2}} \right) \\
&= - \int \left( \frac{\partial^{2} \log f(x; \theta)}{\partial^{2} \theta^{2}} \right) f(x; \theta) dx
\end{align*}

\textbf{Theorem 10.18 (Asymptotic Normality of the MLE)}. Under
appropriate regularity conditions, the following hold:
\begin{enumerate}[tightlist,label={\arabic*.}]
\item
  Let \(\SE = \sqrt{1 / I_{n}(\theta)}\). Then,
\end{enumerate}
\[
\frac{\hat{\theta}_{n} - \theta}{\SE } \leadsto N(0, 1)
\]
\begin{enumerate}[tightlist,label={\arabic*.}]
\item
  Let \(\widehat{\SE} = \sqrt{1 / I_{n}(\hat{\theta}_{n})}\). Then,
\end{enumerate}
\[
\frac{\hat{\theta}_{n} - \theta}{\widehat{\SE}} \leadsto N(0, 1)
\]
The first statement says that
\(\hat{\theta}_{n} \approx N(\theta, \SE )\). The second statement
says that this is still true if we replace the standard error
\(\SE \) by its estimated standard error \(\widehat{\SE}\).
Informally this says that the distribution of the MLE can be
approximated with \(N(\theta, \widehat{\SE})\). From this fact we can
construct an asymptotic confidence interval.

\textbf{Theorem 10.19}. Let
\[
C_{n} = \left( \hat{\theta}_{n} - z_{\alpha/2} \widehat{\SE}, \; \hat{\theta}_{n} + z_{\alpha/2} \widehat{\SE} \right)
\]
Then, \(\PROB_\theta(\theta \in C_{n}) \rightarrow 1 - \alpha\) as
\(n \rightarrow \infty\).
\textbf{Proof} Let \(Z\) denote a standard random variable. Then,
\begin{align*}
\PROB_\theta(\theta \in C_{n}) 
&= \PROB_\theta(\hat{\theta}_{n} - z_{\alpha/2} \widehat{\SE} \leq \theta \leq \hat{\theta}_{n} + z_{\alpha/2} \widehat{\SE}) \\
&= \PROB_\theta(-z_{\alpha/2} \leq \frac{\hat{\theta}_{n} - \theta}{\widehat{\SE}} \leq z_{\alpha/2}) \\
&\rightarrow \PROB(-z_{\alpha/2} \leq Z \leq z_{\alpha/2}) = 1 - \alpha
\end{align*}

\subsection*{10.8 Optimality}\label{optimality}
Suppose that \(X_{1}, \dots, X_{n} \sim N(0, \sigma^{2})\). The MLE is
\(\hat{\theta}_{n} = \bar{X}_{n}\). Another reasonable estimator is the
sample median \(\bar{\theta}_{n}\). The MLE satisfies
\[
\sqrt{n}(\hat{\theta}_{n} - \theta) \leadsto N(0, \sigma^{2})
\]
It can be proved that the median satisfies
\[
\sqrt{n}(\bar{\theta}_{n} - \theta) \leadsto N\left(0, \sigma^{2} \frac{\pi}{2} \right)
\]
This means that the median converges to the right value but has a larger
variance than the MLE.
More generally, consider two estimators \(T_{n}\) and \(U_{n}\) and suppose
that
\[
\sqrt{n}(T_{n} - \theta) \leadsto N(0, t^{2}) 
\quad \text{and} \quad 
\sqrt{n}(U_{n} - \theta) \leadsto N(0, u^{2})
\]
We define the \textbf{asymptotic relative efficiency} of U to T by
\(ARE(U, T) = t^{2}/u^{2}\). In the Normal example,
\(ARE(\bar{\theta}_{n}, \hat{\theta}_{n}) = 2 / \pi = 0.63\).

\textbf{Theorem 10.23}. If \(\hat{\theta}_{n}\) is the MLE and
\(\bar{\theta}_{n}\) is any other estimator then
\[
ARE(\bar{\theta}_{n}, \hat{\theta}_{n}) \leq 1
\]
Thus, MLE has the smallest (asymptotic) variance and we say that MLE is
\textbf{efficient} or \textbf{asymptotically optimal}.
The result is predicated over the model being correct -- otherwise the
MLE may no longer be optimal.

\subsection*{10.9 The Delta Method}\label{delta:method}
Let \(\tau = g(\theta)\) where \(g\) is a smooth function. The maximum
likelihood estimator of \(\tau\) is \(\hat{\tau} = g(\hat{\theta})\).

\textbf{Theorem 10.24 (The Delta Method)}. If \(\tau = g(\theta)\) where
\(g\) is differentiable and \(g'(\theta) \neq 0\) then
\[
\frac{\sqrt{n}(\hat{\tau}_{n} - \tau)}{\widehat{\SE}(\hat{\tau})} \leadsto N(0, 1)
\]
where \(\hat{\tau}_{n} = g(\hat{\theta})\) and
\[
\widehat{\SE}(\hat{\tau}_{n}) = |g'(\hat{\theta})| \widehat{\SE} (\hat{\theta}_{n})
\]
Hence, if
\[
C_{n} = \left( \hat{\tau}_{n} - z_{\alpha/2} \widehat{\SE}(\hat{\tau}_{n}), \; \hat{\tau}_{n} + z_{\alpha/2} \widehat{\SE}(\hat{\tau}_{n}) \right)
\]
then \(\PROB_\theta(\tau \in C_{n}) \rightarrow 1 - \alpha\) as
\(n \rightarrow \infty\).

\subsection*{10.10 Multiparameter Models}\label{multiparameter-models}
We can extend these ideas to models with several parameters.
Let \(\theta = (\theta_{1}, \dots, \theta_{n})\) and let
\(\hat{\theta} = (\hat{\theta}_{1}, \dots, \hat{\theta}_{n})\) be the MLE.
Let \(\ell_{n} = \sum_{i=1}^{n} \log f(X_{i}; \theta)\),
\[
H_{j}j = \frac{\partial^{2} \ell_{n}}{\partial \theta_{j}^{2}}
\quad \text{and} \quad
H_{j}k = \frac{\partial^{2} \ell_{n}}{\partial \theta_{j} \partial \theta_{k}}
\]
Define the \textbf{Fisher Information Matrix} by
\[
I_{n}(\theta) = -
\begin{bmatrix}
\EXP_{\theta}(H_{11}) & \EXP_{\theta}(H_{12}) & \cdots & \EXP_{\theta}(H_{1k}) \\
\EXP_{\theta}(H_{21}) & \EXP_{\theta}(H_{22}) & \cdots & \EXP_{\theta}(H_{2k}) \\
\vdots & \vdots & \ddots & \vdots \\
\EXP_{\theta}(H_{k1}) & \EXP_{\theta}(H_{k2}) & \cdots & \EXP_{\theta}(H_{kk})
\end{bmatrix}
\]
Let \(J_{n}(\theta) = I_{n}^{-1}(\theta)\) be the inverse of \(I_{n}\).

\textbf{Theorem 10.27}. Under appropriate regularity conditions,
\[
\sqrt(n)(\hat{\theta} - \theta) \approx N(0, J_{n}(\theta))
\]
Also, if \(\hat{\theta}_{j}\) is the \(j\)-th component of
\(\hat{\theta}\), then
\[
\frac{\sqrt{n}(\hat{\theta_{j}} - \theta_{j})}{\widehat{\SE}_{j}} \approx N(0, 1)
\]
where \(\widehat{\SE}_{j}^{2}\) is the \(j\)-th diagonal element of
\(J_{n}\). The approximate covariance of \(\hat{\theta}_{j}\) and
\(\hat{\theta}_{k}\) is
\(\COV(\hat{\theta}_{j}, \hat{\theta}_{k}) \approx J_{n}(j, k)\).
There is also a multiparameter delta method. Let
\(\tau = g(\theta_{1}, \dots, \theta_{k})\) be a function and let
\[
\nabla g = \begin{pmatrix}
\frac{\partial g}{\partial \theta_{1}} \\
\vdots \\
\frac{\partial g}{\partial \theta_{k}}
\end{pmatrix}
\]
be the gradient of \(g\).

\textbf{Theorem 10.28 (Multiparameter delta method)}. Suppose that
\(\nabla g\) evaluated at \(\hat{\theta}\) is not 0. Let
\(\hat{\tau} = g(\hat{\theta})\). Then
\[
\frac{\sqrt{n}(\hat{\tau} - \tau)}{\widehat{\SE}(\hat{\tau})} \leadsto N(0, 1)
\]
where
\[
\widehat{\SE}(\hat{\tau}) = \sqrt{\left(\hat{\nabla} g \right)^T \hat{J}_{n} \left(\hat{\nabla} g \right)} ,
\]
\(\hat{J}_{n} = J_{n}(\hat{\theta}_{n})\) and \(\hat{\nabla}g\) is
\(\nabla g\) evaluated at \(\theta = \hat{\theta}\).

\subsection*{10.11 The Parametric
Bootstrap}\label{the-parametric-bootstrap}
For parametric models, standard errors and confidence intervals may also
be estimated using the bootstrap. There is only one change. In
nonparametric bootstrap, we sampled \(X_{1}^{*}, \dots, X_{n}^{*}\) from the
empirical distribution \(\hat{F}_{n}\). In the parametric bootstrap we
sample instead from \(f(x; \hat{\theta}_{n})\). Here, \(\hat{\theta}_{n}\)
could be the MLE or the method of moments estimator.

\subsection*{10.12 Technical Appendix}
\paragraph{10.12.1 Proofs}\label{proofs}
\textbf{Proof of Theorem 10.13}. Since \(\hat{\theta}_{n}\) maximizes
\(M_{n}(\theta)\), we have \(M_{n}(\hat{\theta}) \geq M_{n}(\theta_{*})\).
Hence,
\begin{align*}
M(\theta_{*}) - M(\hat{\theta}_{n}) 
&= M_{n}(\theta_{*}) - M(\hat{\theta}_{n}) + M(\hat{\theta}_{*}) - M_{n}(\theta_{*}) \\
&\leq M_{n}(\hat{\theta}) - M(\hat{\theta}_{n}) + M(\theta_{*}) - M_{n}(\theta_{*}) \\
&\leq \sup_\theta | M_{n}(\theta) - M(\theta) |  + M(\theta_{*})  - M_{n}(\theta_{*}) \\
&\xrightarrow{\textrm{P}} 0
\end{align*}
It follows that, for any \(\delta > 0\),
\[
\PROB(M(\hat{\theta}_{n}) < M(\theta_{*}) - \delta) \rightarrow 0
\]
Pick any \(\epsilon > 0\). There exists \(\delta > 0\) such that
\(|\theta - \theta_{*}| \geq \epsilon\) implies that
\(M(\theta) < M(\theta_{*}) - \delta\). Hence,
\[
\PROB(|\hat{\theta}_{n} - \theta_{*}| > \epsilon) \leq 
\PROB\left( M(\hat{\theta}_{n}) < M(\theta_{*}) - \delta \right) \rightarrow 0
\]
\textbf{Lemma 10.31}. The score function satisfies
\[
\EXP[s(X; \theta)] = 0
\]
\textbf{Proof}. Note that \(1 = \int f(x; \theta) dx\). Differentiate
both sides of this equation to get
\begin{align*}
0 &= \frac{\partial}{\partial \theta} \int f(x; \theta)dx = \int \frac{\partial}{\partial \theta} f(x; \theta) dx \\
&= \int \frac{\frac{\partial f(x; \theta)}{\partial \theta}}{f(x; \theta)} f(x; \theta) dx
= \int \frac{\partial \log f(x; \theta)}{\partial \theta} f(x; \theta) dx \\
&= \int s(x; \theta) f(x; \theta) dx = \EXP[s(X; \theta)]
\end{align*}
\textbf{Proof of Theorem 10.18}. Let
\(\ell(\theta) = \log \mathcal{L}(\theta)\). Then
\[
0 = \ell'(\hat{\theta}) \approx \ell'(\theta) + (\hat{\theta} - \theta) \ell''(\theta)
\]
Rearrange the above equation to get
\(\hat{\theta} - \theta = -\ell'(\theta) / \ell''(\theta)\), or
\[
\sqrt{n}(\hat{\theta} - \theta) = \frac{\frac{1}{\sqrt{n}}\ell'(\theta)}{-\frac{1}{n}\ell''(\theta)} = \frac{\text{TOP}}{\text{BOTTOM}}
\]
Let \(Y_{i} = \partial \log f(X_{i}, \theta) / \partial \theta\). From the
previous lemma \(\EXP(Y_{i}) = 0\) and also
\(\VAR(Y_{i}) = I(\theta)\). Hence,
\[
\text{TOP} = n^{-1/2} \sum_{i} Y_{i} = \sqrt{n} \bar{Y} = \sqrt{n} (\bar{Y} - 0) \leadsto W \sim N(0, I)
\]
by the central limit Theorem. Let
\(A_{i} = -\partial^{2} \log f(X_{i}; \theta) / \partial theta^{2}\). Then
\(\EXP(A_{i}) = I(\theta)\) and
\[
\text{BOTTOM} = \bar{A} \xrightarrow{\textrm{P}} I(\theta)
\]
by the law of large numbers. Apply Theorem 6.5 part (e) to conclude that
\[
\sqrt{n}(\hat{\theta} - \theta) \leadsto \frac{W}{I(\theta)} \sim N \left(0, \frac{1}{I(\theta)} \right)
\]
Assuming that \(I(\theta)\) is a continuous function of \(\theta\), it
follows that \(I(\hat{\theta}_{n}) \xrightarrow{\textrm{P}} I(\theta)\). Now
\begin{align*}
\frac{\hat{\theta}_{n} - \theta}{\widehat{\SE}}&= \sqrt{n} I^{1/2}(\hat{\theta}_{n})(\hat{\theta}_{n} - \theta) \\
&= \left\{ \sqrt{n} I^{1/2}(\theta)(\hat{\theta}_{n} - \theta)\right\} \left\{ \frac{I(\hat{\theta}_{n})}{I(\theta)} \right\}^{1/2}
\end{align*}
The first term tends in distribution to \(N(0, 1)\). The second term
tends in probability to 1. The result follows from Theorem 6.5 part (e).
\textbf{Outline of proof of Theorem 10.24}. Write,
\[
\hat{\tau} = g(\hat{\theta}) \approx g(\theta) + (\hat{\theta} - \theta)g'(\theta) = \tau + (\hat{\theta} - \theta)g'(\theta)
\]
Thus,
\[
\sqrt{n}(\hat{\tau} - \tau) \approx \sqrt{n}(\hat{\theta} - \theta)g'(\theta)
\]
and hence
\[
\frac{\sqrt{n}I^{1/2}(\theta)(\hat{\theta} - \theta)}{g'(\theta)} \approx \sqrt{n}I^{1/2}(\theta)(\hat{\theta} - \theta)
\]
Theorem 10.18 tells us that the right hand side tends in distribution to
\(N(0, 1)\), hence
\[
\frac{\sqrt{n}I^{1/2}(\theta)(\hat{\theta} - \theta)}{g'(\theta)} \leadsto N(0, 1)
\]
or, in other words,
\[
\hat{\tau} \approx N(\tau, \SE ^{2}(\hat{\tau}_{n}))
\]
where
\[
\SE ^{2}(\hat{\tau}_{n}) = \frac{(g'(\theta))^{2}}{nI(\theta)}
\]
The result remains true if we substitute \(\hat{\theta}\) for \(\theta\)
by Theorem 6.5 part (e).
\paragraph{10.12.2 Sufficiency}\label{sufficiency}
A \textbf{statistic} is a function \(T(X^{n})\) of the data. A sufficient
statistic is a statistic that contains all of the information in the
data.
Write \(x^{n} \leftrightarrow y^{n}\) if
\(f(x^{n}; \theta) = c f(y^{n}; \theta)\) for some constant \(c\) that might
depend on \(x^{n}\) and \(y^{n}\) but not \(\theta\). A statistic is
\textbf{sufficient} if \(T(x^{n}) \leftrightarrow T(y^{n})\) implies that
\(x^{n} \leftrightarrow y^{n}\).
Notice that if \(x^{n} \leftrightarrow y^{n}\) then the likelihood functions
based on \(x^{n}\) and \(y^{n}\) have the same shape. Roughly speaking, a
statistic is sufficient if we can calculate the likelihood function
knowing only \(T(X^{n})\).
A statistic \(T\) is \textbf{minimally sufficient} if it is sufficient
and it is a function of every other sufficient statistic.

\textbf{Theorem 10.36}. \(T\) is minimally sufficient if
\(T(x^{n}) = T(y^{n})\) if and only if \(x^{n} \leftrightarrow y^{n}\).
The usual definition of sufficiency is this: \(T\) is sufficient if the
distribution of \(X^{n}\) given \(T(X^{n}) = t\) does not depend on
\(\theta\).

\textbf{Theorem 10.40 (Factorization Theorem)}. \(T\) is sufficient if
and only if there are functions \(g(t, \theta)\) and \(h(x)\) such that
\(f(x^{n}; \theta) = g(t(x^{n}); \theta)h(x^{n})\).

\textbf{Theorem 10.42 (Rao-Blackwell)}. Let \(\hat{\theta}\) be an
estimator and let \(T\) be a sufficient statistic. Define a new
estimator by
\[
\bar{\theta} = \EXP(\hat{\theta} | T)
\]
Then, for every \(\theta\),
\[
R(\theta, \bar{\theta}) \leq R(\theta, \hat{\theta})
\]
where
\(R(\theta, \hat{\theta}) = \EXP_{\theta}[(\theta - \hat{\theta})^{2}]\)
denote the MSE of an estimator.
\paragraph{10.12.3 Exponential Families}\label{exponential-families}
We say that \(\{f(x; \theta) : \theta \in \Theta\}\) is a
\textbf{one-parameter exponential family} if there are functions
\(\eta(\theta)\), \(B(\theta)\), \(T(x)\) and \(h(x)\) such that
\[
f(x; \theta) = h(x) e^{\eta(\theta)T(x) - B(\theta)}
\]
It is easy to see that \(T(X)\) is sufficient. We call \(T\) the
\textbf{natural sufficient statistic}.
We can rewrite an exponential family as
\[
f(x; \eta) = h(x) e^{\eta T(x) - A(\eta)}
\]
where \(\eta = \eta(\theta)\) is called the \textbf{natural parameter}
and
\[
A(\eta) = \log \int h(x) e^{\eta T(x)} dx
\]
Let \(X_{1}, \dots, X_{n}\) be iid from an exponential family. Then
\(f(x^{n}; \theta)\) is an exponential family:
\[
f(x^{n}; \theta) = h_{n}(x^{n}) e^{\eta(\theta) T_{n}(x^{n}) - B_{n}(\theta)}
\]
where \(h_{n}(x^{n}) = \prod_{i} h(x_{i})\), \(T_{n}(x^{n}) = \sum_{i} T(x_{i})\) and
\(B_{n}(\theta) = nB(\theta)\). This implies that \(\sum_{i} T(X_{i})\) is
sufficient.

\textbf{Theorem 10.47}. Let \(X\) have an exponential family. Then,
\[
\EXP(T(X)) = A'(\eta),
\quad
\VAR(T(X)) = A''(\eta)
\]
If \(\theta = (\theta_{1}, \dots, \theta_{n})\) is a vector, then we say
that \(f(x; \theta)\) has exponential family form if
\[
f(x; \theta) = h(x) \exp \left\{ \sum_{j=1}^{k} \eta_{j}(\theta) T_{j}(x) - B(\theta) \right\}
\]
Again, \(T = (T_{1}, \dots, T_{k})\) is sufficient and \(n\) iid samples
also has exponential form with sufficient statistic
\(\left(\sum_{i} T_{1}(X_{i}), \dots, \sum_{i} T_{k}(X_{i})\right)\).
\section*{10.13 Exercises}

\textbf{Exercise 10.13.1}. Let
\(X_{1}, \dots, X_{n} \sim \text{Gamma}(\alpha, \beta)\). Find the method of
moments estimator for \(\alpha\) and \(\beta\).

\textbf{Solution}.
The first two moments are:
\[
\alpha_{1} = \EXP(X) = \frac{\alpha}{\beta}\\
\alpha_{2} = \EXP(X^{2}) = \VAR(X) + \EXP(X)^{2} = \frac{\alpha}{\beta^{2}} + \frac{\alpha^{2}}{\beta^{2}} = \frac{\alpha(\alpha + 1)}{\beta^{2}} 
\]
We have the sample moments:
\[
\hat{\alpha}_{1} = \frac{1}{n}\sum_{i=1}^{n} X_{i}
\quad \quad
\hat{\alpha}_{2} = \frac{1}{n}\sum_{i=1}^{n} X_{i}^{2}
\]
Equating these we get:
\[
\hat{\alpha}_{1} = \frac{\hat{\alpha}_{n}}{\hat{\beta}_{n}}
\quad \quad
\hat{\alpha}_{2} = \frac{\hat{\alpha}_{n}(\hat{\alpha}_{n} + 1)}{\hat{\beta}_{n}^{2}}
\]
Solving these we get the method of moments estimators for \(\alpha\) and
\(\beta\):
\[
\hat{\alpha}_{n} = \frac{\hat{\alpha}_{1}^{2}}{\hat{\alpha}_{2} - \hat{\alpha}_{1}^{2}}
\quad \quad
\hat{\beta}_{n} = \frac{\hat{\alpha}_{1}}{\hat{\alpha}_{2} - \hat{\alpha}_{1}^{2}}
\]

\textbf{Exercise 10.13.2}. Let
\(X_{1}, \dots, X_{n} \sim \text{Uniform}(a, b)\) where \(a, b\) are unknown
parameters and \(a < b\).
\textbf{(a)} Find the method of moments estimators for \(a\) and \(b\).
\textbf{(b)} Find the MLE \(\hat{a}\) and \(\hat{b}\).
\textbf{(c)} Let \(\tau = \int x dF(x)\). Find the MLE of \(\tau\).
\textbf{(d)} Let \(\hat{\tau}\) be the MLE from the previous item. Let
\(\tilde{\tau}\) be the nonparametric plug-in estimator of
\(\tau = \int x dF(x)\). Suppose that \(a = 1\), \(b = 3\) and
\(n = 10\). Find the MSE of \(\hat{\tau}\) by simulation. Find the MSE
of \(\tilde{\tau}\) analytically. Compare.

\textbf{Solution}.
\textbf{(a)}
The first two moments are:
\[
\alpha_{1} = \EXP(X) = \frac{a + b}{2} \\
\alpha_{2} = \EXP(X^{2}) = \VAR(X) + \EXP(X)^{2} = \frac{(b - a)^{2}}{12} + \frac{(a + b)^{2}}{4}
= \frac{a^{2} + ab + b^{2}}{3}
\]
We have the sample moments:
\[
\hat{\alpha}_{1} = \frac{1}{n}\sum_{i=1}^{n} X_{i}
\quad \quad
\hat{\alpha}_{2} = \frac{1}{n}\sum_{i=1}^{n} X_{i}^{2}
\]
Equating these we get:
\[
\hat{\alpha}_{1} = \frac{\hat{a} + \hat{b}}{2}
\quad \quad
\hat{\alpha}_{2} = \frac{(\hat{b} - \hat{a})^{2}}{12} + \frac{(\hat{a} + \hat{b})^{2}}{4}
\]
Solving these we get the method of moment estimators for \(a\) and
\(b\):
\[
\hat{a} = \hat{\alpha}_{1} - \sqrt{3}(\hat{\alpha}_{1}^{2} - \hat{\alpha}_{2})
\quad \quad
\hat{b} = \hat{\alpha}_{1} + \sqrt{3}(\hat{\alpha}_{1}^{2} - \hat{\alpha}_{2})
\]
\textbf{(b)}
The probability density function for each \(X_{i}\) is
\[
f(x; (a, b)) = \begin{cases}
(b - a)^{-1} & \text{if } a \leq x \leq b \\
0 & \text{otherwise}
\end{cases}
\]
The likelihood function is
\[
\mathcal{L}_{n}(a, b) = \prod_{i=1}^{n} f(X_{i}; (a, b)) = \begin{cases}
(b-a)^{-n} & \text{if } a \leq X_{i} \leq b \text{ for all } X_{i}\\
0 & \text{otherwise}
\end{cases}
\]
The parameters that maximize the likelihood function make the \(b - a\)
as small as possible -- that is, we should pick the maximum \(a\) and
the minimum \(b\) for which the likelihood function is non-zero. So the
MLEs are:
\[
\hat{a} = \min \{X_{1}, \dots, X_{n} \}
\quad \quad
\hat{b} = \max \{X_{1}, \dots, X_{n} \}
\]
\textbf{(c)}
\(\tau = \int x dF(x) = \EXP(x) = (a + b)/2\), so since the MLE is
equivariant, the MLE of \(\tau\) is
\[
\hat{\tau} = \frac{\hat{a} + \hat{b}}{2} = \frac{\min \{X_{1}, \dots, X_{n}\} + \max\{X_{1}, \dots, X_{n}\}}{2}
\]
\textbf{(d)}

\begin{python}
import numpy as np
a = 1
b = 3
n = 10
X = np.random.uniform(low=a, high=b, size=n)
\end{python}

\begin{python}
tau_hat = (X.min() + X.max()) / 2
# Nonparametric bootstrap to find MSE of tau_hat
B = 10000
t_boot = np.empty(B)
for i in range(B):
    xx = np.random.choice(X, n, replace=True)
    t_boot[i] = (xx.min() + xx.max()) / 2
    
se = t_boot.std()
print("MSE for tau_hat: \t %.3f" % se)
\end{python}
\begin{console}
MSE for tau\_hat:         0.150
\end{console}
Analytically, we have:
\begin{align*}
\VAR(\tilde{\tau}) 
&= \EXP(\tilde{\tau}^{2}) - (\EXP(\tilde{\tau}))^{2} \\
&= \frac{1}{n^{2}}\left(\EXP\left[ \left(\sum_{i=1}^{n} X_{i}\right)^{2}\right] - \left(\EXP\left[\sum_{i=1}^{n} X_{i} \right]\right)^{2}\right) \\
&= \frac{1}{n^{2}}\left( \EXP\left[ \sum_{i=1}^{n} X_{i}^{2} + \sum_{i=1}^{n} \sum_{j=1, j \neq i}^{n} X_{i} X_{j} \right] - \left(n \frac{a + b}{2}\right)^{2}\right) \\
&= \frac{1}{n^{2}}\left( \sum_{i=1}^{n} \EXP[X_{i}^{2}] + \sum_{i=1}^{n} \sum_{j=1, j \neq i}^{n} \EXP[X_{i}]\EXP[X_{j}]  - \left(n \frac{a + b}{2}\right)^{2}\right) \\
&= \frac{1}{n^{2}}\left( n \frac{a^{2} + ab + b^{2}}{3} + n(n-1) \left(\frac{a+b}{2}\right)^{2}  - n^{2}\left(\frac{a + b}{2}\right)^{2}\right) \\
&= \frac{1}{n^{2}}\left( n \frac{a^{2} + ab + b^{2}}{3} - n \left(\frac{a+b}{2}\right)^{2} \right) \\
&= \frac{1}{n} \left( \frac{a^{2} + ab + b^{2}}{3} - \frac{a^{2} + 2ab + b^{2}}{4}\right) \\
&= \frac{1}{n} \frac{(b - a)^{2}}{12}
\end{align*}
Therefore,
\[
\SE (\tilde{\tau}) = \sqrt{\frac{1}{n} \frac{(b - a)^{2}}{12}}
\]

\begin{python}
se_tau_tilde = np.sqrt((1/n) * ((b - a)**2 / 12))
print("MSE for tau_tilde: \t %.3f" % se_tau_tilde)
\end{python}
\begin{console}
MSE for tau\_tilde:       0.183
\end{console}

\textbf{Exercise 10.13.3}. Let
\(X_{1}, \dots, X_{n} \sim N(\mu, \sigma^{2})\). Let \(\tau\) be the 0.95
percentile, i.e.~\(\PROB(X < \tau) = 0.95\).
\textbf{(a)} Find the MLE of \(\tau\).
\textbf{(b)} Find an expression for an approximate \(1 - \alpha\)
confidence interval for \(\tau\).
\textbf{(c)} Suppose the data are:
\[
\begin{matrix}
3.23 & -2.50 &  1.88 & -0.68 &  4.43 & 0.17 \\ 
1.03 & -0.07 & -0.01 &  0.76 &  1.76 & 3.18 \\
0.33 & -0.31 &  0.30 & -0.61 &  1.52 & 5.43 \\
1.54 &  2.28 &  0.42 &  2.33 & -1.03 & 4.00 \\
0.39 
\end{matrix}
\]
Find the MLE \(\hat{\tau}\). Find the standard error using the delta method. Find the standard error using the parametric bootstrap.

\textbf{Solution}.
\textbf{(a)}
Let \(Z \sim N(0, 1)\), so \((X - \mu) / \sigma \sim Z\). We have:
\begin{align*}
\PROB(X < \tau) &= 0.95 \\
\PROB\left(\frac{X - \mu}{\sigma} < \frac{\tau - \mu}{\sigma}\right) &= 0.95 \\
\PROB\left(Z < \frac{\tau - \mu}{\sigma}\right) &= 0.95 \\
\frac{\tau - \mu}{\sigma} &= z_{5\%} \\
\tau &= \mu + z_{5\%} \sigma 
\end{align*}
Since the MLE is equivariant,
\(\hat{\tau} = \hat{\mu} + z_{5\%} \hat{\sigma}\), where
\(\hat{\mu}, \hat{\sigma}\) are the MLEs for the Normal distribution
parameters:
\[
\hat{\mu} = n^{-1} \sum_{i=1}^{n} X_{i}
\quad \quad
\hat{\sigma} = \sqrt{n^{-1} \sum_{i=1}^{n} (X_{i} - \bar{X})^{2}}
\]
\textbf{(b)}
Use the multiparameter delta method.
We have \(\tau = g(\mu, \sigma) = \mu + z_{5\%} \sigma\), so
\[
\nabla g = \begin{bmatrix}
\partial g / \partial \mu \\
\partial g / \partial \sigma
\end{bmatrix}
= \begin{bmatrix}
1 \\
z_{5\%}
\end{bmatrix}
\].
The Fisher Information Matrix for the Normal process is
\[
I_{n}(\mu, \sigma) = \begin{bmatrix}
n / \sigma^{2} & 0 \\
0 & 2n / \sigma^{2}
\end{bmatrix}
\]
then its inverse is
\[
J_{n} = I_{n}^{-1}(\mu, \sigma) = \frac{1}{n} \begin{bmatrix}
\sigma^{2} & 0 \\
0 & \sigma^{2}/2
\end{bmatrix}
\]
and the standard error estimate for our new parameter variable is
\[
\widehat{\SE}(\hat{\tau}) = \sqrt{(\hat{\nabla} g)^T \hat{J}_{n} (\hat{\nabla} g)} = \hat{\sigma} \sqrt{n^{-1}(1 + z_{5\%}^{2} / 2)}
\]
A \(1 - \alpha\) confidence interval for \(\hat{\tau}\), then, is
\[
C_{n} = \left(
\hat{\mu} + \hat{\sigma}\left( z_{5\%} - z_{\alpha / 2} \sqrt{n^{-1}(1 + z_{5\%}^{2} / 2)} \right), \;
\hat{\mu} + \hat{\sigma}\left( z_{5\%} + z_{\alpha / 2} \sqrt{n^{-1}(1 + z_{5\%}^{2} / 2)} \right) \right)
\]
\textbf{(c)}

\begin{python}
import numpy as np
from scipy.stats import norm
z_05 = norm.ppf(0.95)
z_025 = norm.ppf(0.975)
\end{python}

\begin{python}
X = np.array([
    3.23, -2.50,  1.88, -0.68,  4.43, 0.17,
    1.03, -0.07, -0.01,  0.76,  1.76, 3.18,
    0.33, -0.31,  0.30, -0.61,  1.52, 5.43,
    1.54,  2.28,  0.42,  2.33, -1.03, 4.00,
    0.39   
])
# Estimate the MLE tau_hat
n = len(X)
mu_hat = X.mean()
sigma_hat = X.std()
tau_hat = mu_hat + z_05 * sigma_hat
print("Estimated tau: %.3f" % tau_hat)
\end{python}
\begin{console}
Estimated tau: 4.180
\end{console}

\begin{python}
# Confidence interval using delta method
se_tau_hat = sigma_hat * np.sqrt((1/n) * (1 + z_05 * z_05 / 2))
confidence_interval = (tau_hat - z_025 * se_tau_hat, 
                         tau_hat + z_025 * se_tau_hat)
print("Estimated tau (delta method, 95%% confidence interval): 
       \t (%.3f, %.3f)" % confidence_interval)
\end{python}
\begin{console}
Estimated tau (delta method, 95\% confidence interval):   (3.088, 5.273)
\end{console}

\begin{python}
# Confidence interval using parametric bootstrap
n = len(X)
mu_hat = X.mean()
sigma_hat = X.std()
tau_hat = mu_hat + z_05 * sigma_hat
B = 10000
t_boot = np.empty(B)
for i in range(B):
    xx = norm.rvs(loc=mu_hat, scale=sigma_hat, size=n)
    t_boot[i] = np.quantile(xx, 0.95)
    
se_tau_hat_bootstrap = t_boot.std()
confidence_interval = (tau_hat - z_025 * se_tau_hat_bootstrap, 
                         tau_hat + z_025 * se_tau_hat_bootstrap)
print("Estimated tau (parametric bootstrap, 95%% confidence interval): 
      \t (%.3f, %.3f)" % confidence_interval)
\end{python}
\begin{console}
Estimated tau (parametric bootstrap, 95\% confidence interval):
                                                 (2.887, 5.474)
\end{console}

\textbf{Exercise 10.13.4} Let
\(X_{1}, \dots, X_{n} \sim \text{Uniform}(0, \theta)\). Show that the MLE is
consistent.
Hint: Let \(Y = \max \{ X_{1}, \dots, X_{n} \}\). For any c,
\(\PROB(Y < c) = \PROB(X_{1} < c, X_{2} < c, \dots, X_{n} < c) = \PROB(X_{1} < c)\PROB(X_{2} < c)\dots\PROB(X_{n} < c)\).

\textbf{Solution}.
The probability density function is
\[
f(x, \theta) = \PROB(Y < x) = \prod_{i = 1}^{n} \PROB(X_{i} < x) = f_{\text{Uniform}(0, \theta)}(x)^{n}
\]
The probability density function for the original distribution is
\[
f_{\text{Uniform}(0, \theta)}(x) = \begin{cases}
\theta^{-1} & \text{if } 0 \leq x \leq \theta \\
0 & \text{otherwise}
\end{cases}
\]
so
\[
f(x, \theta) = \begin{cases}
\theta^{-n} & \text{if } 0 \leq x \leq \theta \\
0 & \text{otherwise}
\end{cases}
\]
The likelihood is maximized when \(\theta\) is as small as possible
while keeping all samples within the first case, so
\(\hat{\theta}_{n} = \max \{X_{1}, \dots, X_{n} \}\).
For a given \(\epsilon > 0\), we have
\[
\PROB(\hat{\theta}_{n} < \theta - \epsilon) = \prod_{i=1}^{n} \PROB(X_{i} < \theta - \epsilon) = \left(1 - \frac{\epsilon}{\theta} \right)^{n}
\]
which goes to 0 as \(n \rightarrow \infty\), so
\(\lim _{n \rightarrow \infty} \hat{\theta}_{n} = \theta\), and thus the
MLE is consistent.

\textbf{Exercise 10.13.5}. Let
\(X_{1}, \dots, X_{n} \sim \text{Poisson}(\lambda)\). Find the method of
moments estimator, the maximum likelihood estimator, and the Fisher
information \(I(\lambda)\).

\textbf{Solution}.
The first moment is:
\[
\EXP(X) = \lambda
\]
We have the sample moment:
\[
\hat{\alpha}_{1} = \frac{1}{n} \sum_{i=1}^{n} X_{i}
\]
Equating those, the method of moments estimator for \(\hat{\lambda}\)
is:
\[
\hat{\lambda} = \hat{\alpha}_{1} = \frac{1}{n} \sum_{i=1}^{n} X_{i}
\]
The likelihood function is
\[
\mathcal{L}_{n}(\lambda) = \prod_{i=1}^{n} f(X_{i}; \lambda) = \prod_{i=1}^{n} \frac{\lambda^{X_{i}}e^{-\lambda}}{(X_{i})!}
\]
so the log-likelihood function is
\begin{align*}
\ell_{n}(\lambda) 
  = \log \mathcal{L}_{n}(\lambda) 
& = \sum_{i=1}^{n} (\log(\lambda^{X_{i}}e^{-\lambda}) - \log X_{i}!)
\\
& = \sum_{i=1}^{n} (X_{i} \log \lambda - \lambda - \log X_{i}!)
\\
& = -n \lambda + (\log \lambda) \sum_{i=1}^{n} X_{i} - \sum_{i=1}^{n} \log X_{i}!
\end{align*}
To find the MLE, we differentiate this equation with respect to 0 and
equate it to 0:
\[
\frac{\partial \ell_{n}(\lambda)}{\partial \lambda} = 0 \\
-n + \frac{\sum_{i=1}^{n} X_{i}}{\hat{\lambda}} = 0 \\
\hat{\lambda} = \frac{1}{n} \sum_{i=1}^{n} X_{i}
\]
The score function is:
\[
s(X; \lambda) = \frac{\partial \log f(X; \lambda)}{\partial \lambda} = \frac{X}{\lambda} - 1
\]
and the Fisher information is:
\[
I_{n}(\lambda) = \sum_{i=1}^{n} \VAR\left( s(X_{i}; \lambda) \right) 
= \sum_{i=1}^{n} \VAR \left( \frac{X_{i}}{\lambda} - 1 \right)
= \frac{1}{\lambda^{2}}  \sum_{i=1}^{n} \VAR(X_{i}) = \frac{n}{\lambda}
\]
In particular, \(I(\lambda) = I_{1}(\lambda) = 1 / \lambda\).

\textbf{Exercise 10.13.6}. Let \(X_{1}, \dots, X_{n} \sim N(\theta, 1)\).
Define
\[
Y_{i} = \begin{cases}
1 & \text{if } X_{i} > 0 \\
0 & \text{if } X_{i} \leq 0
\end{cases}
\]
Let \(\psi = \PROB(Y_{1} = 1)\).
\textbf{(a)} Find the maximum likelihood estimate \(\hat{\psi}\) of
\(\psi\).
\textbf{(b)} Find an approximate 95\% confidence interval for \(\psi\).
\textbf{(c)} Define \(\bar{\psi} = (1 / n) \sum_{i} Y_{i}\). Show that
\(\bar{\psi}\) is a consistent estimator of \(\psi\).
\textbf{(d)} Compute the asymptotic relative efficiency of
\(\bar{\psi}\) to \(\hat{\psi}\). Hint: Use the delta method to get
the standard error of the MLE. Then compute the standard error (i.e.~the
standard deviation) of \(\bar{\psi}\).
\textbf{(e)} Suppose that the data are not really normal. Show that
\(\psi\) is not consistent. What, if anything, does \(\hat{\psi}\)
converge to?

\textbf{Solution}.
Note that, from the definition,
\(Y_{1}, \dots, Y_{n} \sim \text{Bernoulli}(\Phi(\theta))\), where \(\Phi\)
is the CDF for the normal distribution. Let \(p = \Phi(\theta)\).
\textbf{(a)} We have \(\psi = \PROB(Y_{1} = 1) = p\), so the MLE is
\(\hat{\psi} = \hat{p} = \Phi(\hat{\theta})
= \Phi(\bar{X})\), where
\(\bar{X} = n^{-1} \sum_{i=1}^{n} X_{i}\).
\textbf{(b)} Let \(g(\theta) = \Phi(\theta)\). Then
\(g'(\theta) = \phi(\theta)\), where \(\phi\) is the standard normal
PDF. By the delta method,
\(\widehat{\SE}(\hat{\psi}) = |g'(\hat{\theta})| \widehat{\SE}(\hat{\theta}) = \phi(\bar{X}) n^{-1/2}\).
Then, an approximate 95\% confidence interval is
\[
C_{n} = \left(\Phi(\bar{X}) \left(1 - \frac{z_{2.5\%}}{\sqrt{n}}\right), \; 
\Phi(\bar{X}) \left(1 + \frac{z_{2.5\%}}{\sqrt{n}}\right) \right)
\]
\textbf{(c)} \(\bar{\psi}\) has mean \(p\), so consistency follows
from the law of large numbers.
\textbf{(d)} We have \(\VAR(Y_{1}) = \psi (1 - \psi)\), since
\(Y_{1}\) follows a Bernoulli distribution, so
\(\VAR(\bar{\psi}) = \VAR(Y_{1}) / n = \psi (1 - \psi) / n\).
From (b), \(\VAR{\hat{\psi}} = \phi(\theta) / n\).
Therefore, the asymptotic relative efficiency is
\[
\frac{\psi(1 - \psi)}{\phi(\theta)} = \frac{\Phi(\theta)(1 - \Phi(\theta))}{\phi(\theta)}
\]
\textbf{(e)} By the law of large numbers, we still have that
\(\bar{X}\) converges to
\(\EXP(Y_{1}) = \PROB(Y_{1} = 1) \cdot 1 + \PROB(Y_{1} = 0)\cdot 0 = \PROB(Y_{1} = 1) = 1 - F_X(0) = \mu_Y\).
Then \(\hat{\psi} = \Phi(\bar{X})\) converges to \(\Phi(\mu_Y)\).
But the true value of \(\psi\) is \(\PROB(Y_{1} = 1) = 1 - F_X(0)\).
But for an arbitrary distribution \(1 - F_X(0) \neq \Phi(1 - F_X(0))\).

\textbf{Exercise 10.13.7}. (Comparing two treatments). \(n_{1}\) people
are given treatment 1 and \(n_{2}\) people are given treatment 2. Let
\(X_{1}\) be the number of people on treatment 1 who respond favorably to
the treatment and let \(X_{2}\) be the number of people on treatment 2 who
respond favorably. Assume that \(X_{1} \sim \text{Binomial}(n_{1}, p_{1})\),
\(X_{2} \sim \text{Binomial}(n_{2}, p_{2})\). Let \(\psi = p_{1} - p_{2}\).
\textbf{(a)} Find the MLE of \(\psi\).
\textbf{(b)} Find the Fisher Information Matrix \(I(p_{1}, p_{2})\).
\textbf{(c)} Use the multiparameter delta method to find the asymptotic
standard error of \(\hat{\psi}\).
\textbf{(d)} Suppose that \(n_{1} = n_{2} = 200\), \(X_{1} = 160\) and
\(X_{2} = 148\). Find \(\hat{\psi}\). Find an approximate 90\% confidence
interval for \(\psi\) using (i) the delta method and (ii) the parametric
bootstrap.

\textbf{Solution}.
\textbf{(a)} The MLE is equivariant, so
\[
\hat{\psi} = \hat{p}_{1} - \hat{p}_{2} = \frac{X_{1}}{n_{1}} - \frac{X_{2}}{n_{2}}
\]
\textbf{(b)}
The probability mass function is
\[
f((x_{1}, x_{2}); \psi) = f_{1}(x_{1}; p_{1}) f_{2}(x_{2}; p_{2}) = 
\binom{n_{1}}{x_{1}} p_{1}^{x_{1}} (1 - p_{1})^{n_{1} - x_{1}}
\binom{n_{2}}{x_{2}} p_{2}^{x_{2}} (1 - p_{2})^{n_{2} - x_{2}}
\]
The log likelihood is
\begin{align*}
\ell_{n} &= \log f((x_{1}, x_{2}); \psi) 
\\
&= \sum_{i=1}^{2} \log \binom{n_{i}}{x_{i}} + x_{i} \log p_{i} + (n_{i} - x_{i}) \log (1 - p_{i})
\end{align*}
Calculating the partial derivatives and their expectations,
\begin{align*}
H_{11} & = \frac{\partial^{2} \ell_{n}}{\partial p_{1}^{2}}
= \frac{\partial}{\partial p_{1}} \left( \frac{x_{1}}{p_{1}} - \frac{n_{1} - x_{1}}{1 - p_{1}}\right)
= -\frac{x_{1}}{p_{1}^{2}} - \frac{n_{1} - x_{1}}{(1 - p_{1})^{2}} \\
\EXP[H_{11}] &= -\frac{\EXP[x_{1}]}{p_{1}^{2}} - \frac{\EXP[n - x_{1}]}{(1 - p_{1})^{2}}
= -\frac{n_{1} / p_{1}}{p_{1}^{2}} - \frac{n_{1}/(1 - p_{1})}{(1 - p_{1})^{2}}
= -\frac{n_{1}}{p_{1}} - \frac{n_{1}}{1 - p_{1}} = -\frac{n_{1}}{p_{1}(1 - p_{1})}
\end{align*}
\begin{align*}
H_{22} &= -\frac{x_{2}}{p_{2}^{2}} - \frac{n_{2} - x_{2}}{(1 - p_{2})^{2}} \\
\EXP[H_{22}] &= -\frac{n_{2}}{p_{2}(1 - p_{2})}
\end{align*}
\[
H_{12} = \frac{\partial^{2} \ell_{n}}{\partial p_{1} \partial p_{2}} = 0
\]
\[
H_{21} = 0
\]
So the Fisher Information Matrix is:
\[
I(p_{1}, p_{2}) = \begin{bmatrix}
\frac{n_{1}}{p_{1}(1 - p_{1})} & 0\\
0 & \frac{n_{2}}{p_{2}(1 - p_{2})}
\end{bmatrix}
\]
\textbf{(c)} Using the multiparameter delta method,
\(g(\psi) = p_{1} - p_{2}\), so
\[
\nabla g = \begin{bmatrix}
\partial g / \partial p_{1} \\ \partial g / \partial p_{2}
\end{bmatrix}
= \begin{bmatrix}
1 \\ -1
\end{bmatrix}
\]
The inverse of the Fisher Information Matrix is
\[
J(p_{1}, p_{2}) = I(p_{1}, p_{2})^{-1} = \begin{bmatrix}
\frac{p_{1}(1 - p_{1})}{n_{1}} & 0 \\
0 & \frac{p_{2}(1 - p_{2})}{n_{2}}
\end{bmatrix}
\]
Then the asymptotic standard error of \(\hat{\psi}\) is:
\[
\widehat{\SE}(\hat{\psi}) = \sqrt{(\hat{\nabla} g)^T \hat{J}_{n} (\hat{\nabla} g)}
= \sqrt{\frac{p_{1}(1 - p_{1})}{n_{1}} + \frac{p_{2}(1 - p_{2})}{n_{2}}}
\]
\textbf{(d)}

\begin{python}
import numpy as np
from scipy.stats import norm, binom
n = 200
X1 = 160
X2 = 148
\end{python}

\begin{python}
p1_hat = X1 / n
p2_hat = X2 / n
psi_hat = p1_hat - p2_hat
print("Estimated psi: \t %.3f" % psi_hat)
\end{python}
\begin{console}
Estimated psi:   0.060
\end{console}

\begin{python}
# Confidence using delta method
z = norm.ppf(.95)
se_delta = np.sqrt(p1_hat * (1 - p1_hat)/n + p2_hat * (1 - p2_hat) / n)
confidence_delta = (psi_hat - z * se_delta, psi_hat + z * se_delta)
print("90%% confidence interval (delta method): \t %.3f, %.3f" % confidence_delta)
\end{python}
\begin{console}
90\% confidence interval (delta method):          -0.009, 0.129
\end{console}

\begin{python}
# Confidence using parametric bootstrap
B = 1000
xx1 = binom.rvs(n, p1_hat, size=B)
xx2 = binom.rvs(n, p2_hat, size=B)
t_boot = xx1 / n - xx2 / n
se_bootstrap = t_boot.std()
confidence_delta = (psi_hat - z * se_bootstrap, psi_hat + z * se_bootstrap)
print("90%% confidence interval (parametric bootstrap): \t %.3f, %.3f" % confidence_delta)
\end{python}
\begin{console}
90\% confidence interval (parametric bootstrap):          -0.010, 0.130
\end{console}

\textbf{Exercise 10.13.8}. Find the Fisher information matrix for
Example 10.29:
Let \(X_{1}, \dots, X_{n} \sim N(\mu, \sigma^{2})\).

\textbf{Solution} The log likelihood is:
\[
\ell_{n} = \sum_{i} \log f(x; (\mu, \sigma))
= n \left[ \log \left( \frac{1}{\sigma \sqrt{2 \pi}} \right) + \left( -\frac{1}{2} \left(\frac{x - \mu}{\sigma} \right)^{2}\right) \right]
\]
From this,
\[
H_{11} = \frac{\partial^{2} \ell_{n}}{\partial \mu^{2}} = -\frac{n}{\sigma^{2}}
\]
\[
H_{22} = \frac{\partial^{2} \ell_{n}}{\partial \sigma^{2}} = -\frac{n}{\sigma^{2}} - \frac{n}{\sigma^{2}} = -\frac{2n}{\sigma^{2}}
\]
\[
H_{12} = H_{21} = \frac{\partial^{2} \ell_{n}}{\partial \mu \partial \sigma} = 0
\]
So the Fisher Information Matrix is
\[
I(\mu, \sigma) = -\begin{bmatrix}
\EXP[H_{11}] & \EXP[H_{12}] \\
\EXP[H_{21}] & \EXP[H_{22}]
\end{bmatrix} = \begin{bmatrix}
\frac{n}{\sigma^{2}} & 0 \\
0 & \frac{2n}{\sigma^{2}}
\end{bmatrix}
\]

\textbf{Exercises 10.13.9 and 10.13.10}. See final exercises from
chapter 9.
